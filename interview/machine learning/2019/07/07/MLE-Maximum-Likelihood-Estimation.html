<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>MLE: Maximum Likelihood Estimation | RoundTable</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MLE: Maximum Likelihood Estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="liklihood란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D \mid W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) ." />
<meta property="og:description" content="liklihood란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D \mid W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) ." />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html" />
<meta property="og:site_name" content="RoundTable" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-07T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html"},"description":"liklihood란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를 의미합니다. 수식으로 나타내면, $P(D \\mid W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) .","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html","headline":"MLE: Maximum Likelihood Estimation","dateModified":"2019-07-07T00:00:00-05:00","datePublished":"2019-07-07T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="RoundTable" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">RoundTable</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/">Posts</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">MLE: Maximum Likelihood Estimation</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-07T00:00:00-05:00" itemprop="datePublished">
        Jul 7, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/FastPages/categories/#interview">interview</a>
        &nbsp;
      
        <a class="category-tags-link" href="/FastPages/categories/#machine learning">machine learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#classification-mle와-kl-divergence-cross-entropy">Classification: MLE와 KL-divergence, Cross Entropy</a></li>
<li class="toc-entry toc-h2"><a href="#regression-mle와-method-of-least-squares">Regression: MLE와 Method of Least Squares</a></li>
<li class="toc-entry toc-h1"><a href="#map-maximum-a-posterior-estimation">MAP: Maximum a Posterior Estimation</a>
<ul>
<li class="toc-entry toc-h2"><a href="#bayesian-interpretation-of-ridge-regularization">Bayesian interpretation of Ridge regularization</a></li>
</ul>
</li>
</ul><p><em>liklihood</em>란, 이미 주어진 표본적 증거로 비추어보았을 때, 모집단에 관해 어떠한 통계적 추정이 그럴듯한 정도를  의미합니다. 수식으로 나타내면, $P(D \mid W)$ 처럼 나타낼 수 있다. (D = observation, W = parameters) .</p>

<p>동전던지기 예시를 생각해보면, 쉽게 이해할 수 있다. 일반적으로 동전 던지기를 해서 앞면이 나올 확률은 0.5라고 생각합니다. 하지만, 이는 우리가 가정한 값이지 실제의 값은 아닙니다. 이런 이유로 몇 번의 수행결과로 동전의 앞면이 나올 확률 $P(H)$를 정의하고자 합니다.</p>

<p>만약 동전을 100번 던졌을 때, 동전의 앞면이 56번 나왔다면 ‘동전의 앞면이 나올 확률’은 몇이라고 얘기할 수 있을까?  이 문제의 해답이 <strong>Maximum Likelihood</strong>를 구하는 것이다. 즉, observation이 주어졌을 때, 가장 그럴듯한 가설(혹은 parameter)를 찾는 문제가 <strong>Maximum Likelihood Estimation</strong>이다.</p>

<p>동전 던지기는 이항분포를 따릅니다. x는 100번 던졌을 때 앞면이 나온 횟수를 의미하고 p는 앞면이 나올 확률을 의미합니다.
<script type="math/tex">p(x) = \begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x}</script>
likelihood는 다음과 같이 정의됩니다. ($X$는 앞면이 나온 횟수, $\theta$는 앞면이 나올 확률)
<script type="math/tex">P(X \mid \theta)</script>
아래는 likelihood의 그래프입니다.</p>

<p><img src="/FastPages/images/2019-07-07-MLE%20Maximum%20Likelihood%20Estimation/binomial.png" alt="" title="Binomial distribution for coin toss"></p>

<p>앞면이 나올 확률이 0. 56인 지점에서 가장 높은 likelihood를 가지고 있음을 알 수 있습니다.</p>

<script type="math/tex; mode=display">\hat{\theta} = argmax_{\theta}P(X \mid \theta)</script>

<script type="math/tex; mode=display">\hat{\theta} = 0.56</script>

<h2 id="classification-mle와-kl-divergence-cross-entropy">
<a class="anchor" href="#classification-mle%EC%99%80-kl-divergence-cross-entropy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification: MLE와 KL-divergence, Cross Entropy</h2>

<p>KL-divergence는 다음과 같이 정의됩니다.</p>

<script type="math/tex; mode=display">D_{KL}(P \rVert Q) = -\sum_{x \in X}P(x)\log{\frac{Q(x)}{P(x)}}</script>

<script type="math/tex; mode=display">D_{KL}(P \rVert Q) = -\sum_{x \in X}P(x)(\log{Q(x)} - \log{P(x)})</script>

<p>즉, Q distribution과 P distribution간의 차이를 나타내주는 역할을 합니다. 만약 두 분포간의 차이를 나타내는데 필요한 정보량이 많다면, 두 분포는 차이가 크다고 할 수 있습니다.</p>

<p>Cross entropy는 다음과 같이 정의 됩니다.</p>

<script type="math/tex; mode=display">H(p, q) = -\sum_{x \in X}p(x)\log{q(x)} = E_p[-\log{q}]</script>

<p>실제 분포 $p$에 대해서 $-\log{q}$를 평균내는 것입니다. 만약, $q$가  p와 유사할 수록 cross entropy는 낮게 나올 것입니다.  사실 cross entropy는 kl-divergence에서 출발했다고 해석할 수도 있습니다. 아래 수식을 보면 $\log{P(x)}$는 실제 분포가 주어지면, constant term이 됩니다. 따라서 이를 제외하고 본 것이 cross entropy라고 해석할 수 있습니다.</p>

<script type="math/tex; mode=display">D_{KL}(P \rVert Q) = -\sum_{x \in X}P(x)(\log{Q(x)} - \log{P(x)})</script>

<p>그렇다면, MLE와 KLD, cross entropy는 어떤 관계를 가질까요? 결론부터 말씀드리면, 이들은 본질적으로 같은 일을 합니다. 아래는 수식입니다.
<script type="math/tex">\begin{align}
\theta_{ML} =\arg\max_{\theta}P_{model}(X \mid \theta) = \arg\max_{\theta}E_{X \sim P_{data}}[\log{P_{model}(x \mid \theta)}]
\end{align}</script></p>

<script type="math/tex; mode=display">D_{KL}(P \rVert Q) = E_{x \sim P_{data}}[\log{P_{data}(x)} - \log{P_{model}(x)}]</script>

<script type="math/tex; mode=display">H(p, q) =  E_p[-\log{q}]</script>

<p>KLD를 최소화한다는 것은 결국 cross entropy를 최소화하는 것과 같습니다. 그리고 cross entropy를 최소화 하는 것은 결국 likelihood를 최대화하는 것과 같습니다. 따라서 MLE가 하는 것은 모델이 추정한 데이터의 분포와 실제 데이터의 분포를 가장 유사하게 만들어주는 parameter를 찾는 것이라고 해석할 수 있습니다.</p>

<h2 id="regression-mle와-method-of-least-squares">
<a class="anchor" href="#regression-mle%EC%99%80-method-of-least-squares" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regression: MLE와 Method of Least Squares</h2>

<p>regression task에서는 MLE를 다음과 같이 정의할 수 있습니다.</p>

<script type="math/tex; mode=display">\begin{align}
\theta_{ML} = \arg\max_{\theta} P_{model}(Y \mid X;\theta)=\arg\max_{\theta}\sum_{i=1}^{m}\log{P_{model}(y_i \mid x_i;\theta)}
\end{align}</script>

<p>$P_{model}$이 Gaussian distribution이라고 가정하겠습니다. 위의 log term은 아래와 같이 전개됩니다.</p>

<script type="math/tex; mode=display">\log{P_{model}(y_i \mid x_i;\theta)} = -m\log{\sigma} - \frac{m}{2}\log{2\pi}-\sum_{i=1}^m\frac{\rVert\hat{y}_i - y_i\rVert^2}{2\sigma^2}</script>

<p>$\sigma^2$(분산)은 고정되어 있다고 가정하면, 결국  $\rVert\hat{y}_i - y_i\rVert^2$를 최소화하는 parameter $\theta$를 찾는 것이 MLE입니다. 이는 아래와 같이 결국 least square와 같은 일을 하게 됩니다.</p>

<p>Least squares는 모델에 input을 넣었을때 나오는 output이 실제 target y와 차이를 나타내는 수식이며 이를 가장 작게하는 파라미터 $\theta$를 찾는 것이 관건입니다.
<script type="math/tex">MSE = \frac{1}{m}\sum_{i=1}^m\rVert\hat{y}_i - y_i\rVert ^ 2</script></p>

<h1 id="map-maximum-a-posterior-estimation">
<a class="anchor" href="#map-maximum-a-posterior-estimation" aria-hidden="true"><span class="octicon octicon-link"></span></a>MAP: Maximum a Posterior Estimation</h1>

<p>MAP는 MLE와 다르게, prior라는 assumption을 사용합니다.</p>

<p>posterior $P(w \mid D)$ 는 아래와 같이 정의됩니다. ($P(w)$는 prior $P(D \mid W)$는 likelihood)
<script type="math/tex">P(w \mid D) = \frac{P(D \mid w) P(w)}{P(D)}</script>
위의 동전 던지기 예를 다시 살펴보겠습니다.</p>

<p>만약 동전을 던졌을 때, 앞면이 나올확률이 0.5이라는 가정을 했다고 생각해봅시다. 그리고 실제 100번을 던졌을 때, 앞면이 70번이 나온상황을 보면 posterior는 다음과 같습니다.</p>

<script type="math/tex; mode=display">P(T=0.5 \mid E=0.7)=\frac{P(E=0.7 \mid T=0.5)P(T=0.5)}{P(E=0.7)}</script>

<p>만약 Maximum a posterior estimation을 한다고 하면, prior를 일종의 variable로 설정하고 구할 수 있습니다.</p>

<p><script type="math/tex">P(T=x \mid E=0.7)=\frac{P(E=0.7 \mid T=x)P(T=x)}{P(E=0.7)}</script>
결국은 가장 데이터에 어울리는 prior를 구하는 과정이 MAP라고 할 수 있습니다.</p>

<h2 id="bayesian-interpretation-of-ridge-regularization">
<a class="anchor" href="#bayesian-interpretation-of-ridge-regularization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayesian interpretation of Ridge regularization</h2>

<p><img src="/FastPages/images/2019-07-07-MLE%20Maximum%20Likelihood%20Estimation/lasso_ridge.png" alt="" title="Left: Lasso Right: Ridge"></p>

<p>regression task를 가정했을 때, residual sum of squares(RSS)가 최소화되는 parameter $\beta$를 찾아야합니다. 
<script type="math/tex">\hat{\beta} =\arg\min_{\beta}(y-X\beta)^T(y-X\beta)</script></p>

<p>여기서 ridge regression은 다음과 같이 penalty term의 역할을 합니다. 위의 식과 다르게 RSS와 $\beta$의 크기도 함께 고려해야 하는 것을 의미합니다. ($\rVert\beta\rVert_2^2 = \beta_1^2 + \beta_2^2 + \cdots \beta_p^2$)  결국 model의 complexity를 제한 하는 역할을 하는 것으로 해석할 수 있습니다.</p>

<script type="math/tex; mode=display">\hat{\beta} =\arg \min_{\beta}(y-X\beta)^T(y-X\beta) + \lambda\rVert\beta\rVert_2^2</script>

<p>ridge regression은 bayesian 관점에서 해석할 수 있습니다.  다음은 regression task에 gaussian distribution을 가정한 것입니다.
<script type="math/tex">y \mid X, \beta \sim \mathcal{N}(X\beta, \sigma^2I)</script>
frequentism에서는 $\beta$가 고정된 값이지만, bayesian에서 $\beta$는 prior distribution을 가진다.  $\beta$를 Normal distributioni으로 가정해보겠습니다.
<script type="math/tex">\beta \sim \mathcal{N}(0, \tau^2 I)</script>
posterior는 다음과 같이 전개 됩니다.
<script type="math/tex">% <![CDATA[
\begin{align}
p(\beta \mid y, X)& \varpropto p(\beta)\cdotp(y \mid X, \beta) \\
&\varpropto \exp[-\frac{1}{2}(\beta-0)^T\frac{1}{\tau^2}I(\beta - 0)]\cdot\exp[-\frac{1}{2}(y-X\beta)^T\frac{1}{\sigma^2}(y-X\beta) \\
&=\exp[-\frac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta) -\frac{1}{2\tau^2}\rVert\beta\rVert_2^2]
\end{align} %]]></script>
위의 식을 통해서 maximum a posterior를 구할 수 있습니다.
<script type="math/tex">% <![CDATA[
\begin{align}
\hat{\beta} &= \arg\max_{\beta}\exp[-\frac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta) -\frac{1}{2\tau^2}\rVert\beta\rVert_2^2]\\
&= \arg\min \frac{1}{\sigma^2}(y-X\beta)^T(y-X\beta) + \frac{1}{\tau^2}\rVert\beta\rVert_2^2\\
&= \arg\min (y-X\beta)^T(y-X\beta) + \frac{\sigma^2}{\tau^2}\rVert\beta\rVert_2^2
\end{align} %]]></script>
위의 식을 통해서 $\lambda = \frac{\sigma^2}{\tau^2}$으로 구할 수 있습니다.</p>

<p>참고로 lasso regularization은 prior distribution을 laplace distribution을 사용하면 위와 같이 유도할 수 있습니다.</p>

<p><strong>Reference</strong></p>

<ul>
  <li>https://ratsgo.github.io/statistics/2017/09/23/MLE/</li>
  <li><a href="http://databaser.net/moniwiki/pds/BayesianStatistic/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC%EC%99%80_MLE.pdf">베이즈 정리와 MLE, MAP</a></li>
  <li>https://statisticaloddsandends.wordpress.com/2018/12/29/bayesian-interpretation-of-ridge-regression/</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="RRoundTable/FastPages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/FastPages/interview/machine%20learning/2019/07/07/MLE-Maximum-Likelihood-Estimation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">RoundTable</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">RoundTable</li><li><a class="u-email" href="mailto:ryu071511@gmail.com">ryu071511@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Tech Blog for RoundTable</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
