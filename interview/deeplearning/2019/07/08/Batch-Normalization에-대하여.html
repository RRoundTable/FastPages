<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Batch Normalization에 대하여 | fastpages</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Batch Normalization에 대하여" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Problem Define" />
<meta property="og:description" content="Problem Define" />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-08T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html"},"description":"Problem Define","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html","headline":"Batch Normalization에 대하여","dateModified":"2019-07-08T00:00:00-05:00","datePublished":"2019-07-08T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="fastpages" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/">Posts</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Batch Normalization에 대하여</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-08T00:00:00-05:00" itemprop="datePublished">
        Jul 8, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/FastPages/categories/#interview">interview</a>
        &nbsp;
      
        <a class="category-tags-link" href="/FastPages/categories/#deeplearning">deeplearning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#problem-define">Problem Define</a></li>
<li class="toc-entry toc-h2"><a href="#towards-reducing-internal-covariate-shift">Towards Reducing Internal Covariate Shift</a>
<ul>
<li class="toc-entry toc-h3"><a href="#fixed-distribution">Fixed Distribution</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#normalization-via-mini-batch-statistics">Normalization via Mini-Batch Statistics</a>
<ul>
<li class="toc-entry toc-h3"><a href="#training-and-inference-with-batchnormalized-networks">Training and Inference with BatchNormalized Networks</a></li>
<li class="toc-entry toc-h3"><a href="#batch-normalized-convolutional-networks">Batch-Normalized Convolutional Networks</a></li>
<li class="toc-entry toc-h3"><a href="#batch-normalization-enables-higher">Batch Normalization enables higher</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#batch-normalization-regularizes-the-model">Batch Normalization regularizes the model</a></li>
</ul><h2 id="problem-define">
<a class="anchor" href="#problem-define" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem Define</h2>

<p>학습하는 과정에서 이전 layer의 parameter가 변하면서, 각 layer의 input들의 distribution이 training과정마다 변하게 된다. 이런 문제는 학습이 불안정하게 하며, 낮은 learning rate를 사용해야 학습이 진행된다. 결론적으로는 <strong>saturating non-linearity</strong>의 모델을 학습하기 어려워진다. 이런 현상을 <strong>internal covariate shift</strong> 라고 부른다.</p>

<blockquote>
  <p>saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값이 어떤 범위내에서만 움직이는 것</p>

  <p>ex) sigmoid</p>

  <p>not-saturating non-linearity: 어떤 입력이 무한대로 갈 때 함수값도 무한대로 가는 것을 의미</p>

  <p>ex) Relu</p>
</blockquote>

<p>sigmoid activation에 대해서 생각해보면, 위의 문제가 왜 심각한지 알 수 있다. sigmoid function은 saturating function중 하나로 $\rvert x\rvert $가 증가할 수록 gradient값이 0에 수렴한다.</p>

<script type="math/tex; mode=display">g(x) = \frac{1}{1 + \exp(-x)}</script>

<p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/sigmoid.png" alt="" title="sigmoid"></p>

<p>layer의 depth가 깊어질수록 이 문제는 더 커지게 되는데, 이런 문제를 해결하기 위해서 Relu를 많이 사용한다. 하지만,  Batch normalizing을 사용하게 되면 stable한 distribution을 가지게 되어서 이런 문제를 해결할 수 있다.</p>

<h2 id="towards-reducing-internal-covariate-shift">
<a class="anchor" href="#towards-reducing-internal-covariate-shift" aria-hidden="true"><span class="octicon octicon-link"></span></a>Towards Reducing Internal Covariate Shift</h2>

<p>deep learning model은 많은 layer가 연결되어 있는 구조이다.  layer가 2인 모델을 가정해보면,  다음과 같이 수식으로 나타낼 수 있다.
<script type="math/tex">h_1 = F_1(x)</script></p>

<script type="math/tex; mode=display">output = F_2(h_1)</script>

<p>where $h_1$ is hidden layer, $F_1$ is first layer, $F_2$ is second layer.</p>

<p>위의 구조에서 볼 수 있듯이 $F_2$는 $F_1$의 <strong>dependent</strong> 하다고 할 수 있다. 학습이 진행되는 과정을 보면 internal covariate shift에 대해서 알 수 있다.</p>

<ol>
  <li>첫번째 batch로 output을 구하고 실제 target과의 차이로 loss를 정의한다.</li>
  <li>loss를 바탕으로 gradient를 구한다.</li>
  <li>parameters를 update한다.</li>
</ol>

<p>위의 1 ~ 3번의 과정을 반복하는게 학습의 과정이다. 하지만, 3번의 parameter update과정에서 $F_1, F_2$ layer가 변하게 되는데 이는 distribution이 변하는 것으로 해석할 수 있다. 직관적으로 생각해보면, $F_2$ layer는 update 되기전의 $F_1$을 바탕으로 학습을 진행했는데, 그 다음 step에서 갑자기 변한 $F_1$ layer를 바탕으로 학습을 진행해야 하는 것이다.</p>

<p>이렇게 학습이 진행되면, gradient step은 normalization이 진행되야되는 방향으로 학습이 진행되며 이는 gradient의 효과를 경감시킨다. 이는 아래의 수식 전개를 보면 확인할 수 있다. 해당 수식은 bias parameter만 가진다고 가정한다.</p>

<p>Notation</p>

<ul>
  <li>input: $u$</li>
  <li>learned bias: $b$</li>
  <li>activation computed over training set: $\hat{x} = x - E[x]$</li>
  <li>$x=u +b$, $\mathcal{X} ={ x_{1 \cdots N} }$</li>
  <li>$E[x] = \frac{1}{N}\sum_{i=1}^Nx_i$</li>
</ul>

<p>만약 $E[x]$가 $b$에 미치는 영향을 무시하고 학습한다면, 아래와 같이 update된다.</p>

<script type="math/tex; mode=display">b \leftarrow b + \nabla b</script>

<script type="math/tex; mode=display">\nabla b \varpropto \frac{-\partial l}{\partial\hat{x}}</script>

<p>다음 feed forward 과정을 생각해보면 다음과 같다. ($b$는 update되기전 parameter)
<script type="math/tex">u + (b + \nabla b) - E[u + \nabla b] = u + b - E[u + b]</script>
위의 식에서 볼 수 있듯이 $\nabla b$가 사라짐으로써 학습효과가 없게 된다.</p>

<h3 id="fixed-distribution">
<a class="anchor" href="#fixed-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fixed Distribution</h3>

<p>이런 문제를 해결하기 위해서, 어떤 parameter 값들을 가지든 의도한 distribution이 나오도록 만들어야 한다.  distribution이 고정되면, gradient가 normalization에 dependent하게  만들어진다.</p>

<p>notation</p>

<ul>
  <li>layer input: $x$</li>
  <li>set of inputs over training set: $\mathcal{X}$</li>
  <li>normalization:  $\hat{x} = Norm(x, \mathcal{X})$</li>
</ul>

<p>위의 normalization term은 training example $x$뿐만 아니라 모든 examples $\mathcal{X}$에 영향을 받는다. 만약 $x$가 이전 layer의 output이라면,  $\mathcal{X}$은 이전 layer parameter에 영향을 받는다.</p>

<p>backpropagation 과정에서는 Jacobians를 계산해야한다.</p>

<p>(1) $x$에 대한 gradient
<script type="math/tex">\frac{\partial Norm(x, \mathcal{X})}{\partial x}</script></p>

<p>(2) $\mathcal{X}$ 에 대한 gradient
<script type="math/tex">\frac{\partial Norm(x, \mathcal{X})}{\partial \mathcal{X}}</script></p>

<p>만약 (2)를 고려하지 않게되면, 위에서 언급한 문제가 발생할 수 있다. 하지만 이는 매우 비싼 computation cost를 치뤄야 한다.</p>

<p>[covariance matrix]
<script type="math/tex">Cov[x] = E_{x\in \mathcal{X}}[XX^T] -E[x]E[x]^T</script>
[inverse square root]: to produce the whitened activations
<script type="math/tex">Cov[x]^{-1/2}(x - E[x])</script>
기타 backpropagation과정에서의 derivatives들도 많은 computation cost를 치뤄야한다.</p>

<p>어떻게 하면 합리적인 computation cost로 모델의 representation ability를 보존할 수 있을까?</p>

<h2 id="normalization-via-mini-batch-statistics">
<a class="anchor" href="#normalization-via-mini-batch-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalization via Mini-Batch Statistics</h2>

<p>위에서 언급했듯이, 모든 layer의 input에 대한 full whitening은 computation cost가 높고, 미분가능하지 않을 수도 있다. 이런 문제를 해결하기 위해서 두가지 가정을 한다.</p>

<blockquote>
  <p>[whitening]</p>

  <p>이는 기저벡터(eigenbasis) 데이터를 아이겐밸류(eigenvalue) 값으로 나누어 정규화는 기법이다. 화이트닝 변환의 기하학적 해석은 만약 입력 데이터가 multivariable gaussian 분포를라면 화이트닝된 데이터는 평균은 0이고 공분산(covariance)는 단위행렬을 갖는 정규분포를 갖게된다. 와이트닝은 다음과 같이 구할 수 있다:</p>

  <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="c1"># whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
</span><span class="n">Xwhite</span> <span class="o">=</span> <span class="n">Xrot</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
</code></pre></div>  </div>
</blockquote>

<ol>
  <li>
<strong>layer의 input과 output의 feature를 jointly하게 구하는 것이 아니라, 각 feature를 독립적으로 mean=0, var=1을 가지도록 정규화한다. ($\hat{x}^{(k)}$는 각 layer의 input의  k번째 dimension 성분)</strong>
   <script type="math/tex">\hat{x}^{(k)} = \frac{\hat{x}^{(k)}- E[\hat{x}^{(k)}]}{\sqrt{Var[\hat{x}^{(k)}]}}</script>
   decorrelated feature에도 불구하고 해당 normalization은 convergence 속도를 빠르게 한다고 알려져 있다.(Neural Networks: Tricks of the trade - 1998)</li>
</ol>

<p>하지만, 기억해야 될 것은 normalizing이 layer의 representation능력에 변화를 준다는 것이다. 예를 들어, Sigmoid activation을 사용할 경우, -1 +1 사이로 normalizing이 진행되어 non-linearity의 특성을 잃어버리게 된다.</p>

<ol>
  <li>
    <p><strong>이 문제를 해결하기 위해서는 결국 normalizing이 같은 representation을 하도록 해야한다.</strong></p>

    <p>$\hat{x}^{(k)}$는 activation을 의미하고  $ \gamma^{(k)},  \beta^{(k)}$는 parameter를 의미한다. 이 parameter는 학습시에 모델의 다른 parameter와 같이 학습되며 모델의 representation power를 유지하는 방향으로 학습이 진행된다.
<script type="math/tex">\hat{y}^{(k)} = \gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}</script></p>

    <p>만약 아래와 같이 파라미터 값을 지정한다면, 원래의 representation을 복원할 수 있다.</p>

    <script type="math/tex; mode=display">\gamma^{(k)} = \sqrt{Var[x^{(k)}]}</script>

    <p><script type="math/tex">\beta^{(k)} = E[x^{(k)}]</script>
일반적으로 Stochastic Gradient Training을 하기 때문에 각 mini-batch activation에 해당하는 variance mean를 사용하게 된다. 이는 normalization이 backpropagation과정에 적절히 관여하게 만든다.</p>
  </li>
</ol>

<p>중요한 점은 per-dimension variance를 구하는 것이 computation cost를 낮춘다는 것이다. (Singluar covariance matrices) 아래는 batch normalization algorithm에 대한 설명이다. 주목할 점은 learned parameter $\gamma, \beta$가 training example 뿐 아니라 mini-batch 안에 있는 다른 training example에 영향을 받는다는 것이다. ($\epsilon$은 stability를 위한 constant term이다.)</p>

<p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/Algorithm1.png" alt="" title="Algorithm1"></p>

<p>[Backpropagation 전개]</p>

<p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/backpropagation.png" alt="" title="Backpropagation"></p>

<h3 id="training-and-inference-with-batchnormalized-networks">
<a class="anchor" href="#training-and-inference-with-batchnormalized-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and Inference with BatchNormalized Networks</h3>

<p>Training은 위와 같이 진행하면 되지만, inference를 할 때는 더 적절한 방법이 필요하다. 모델의 output이 input에 deterministic하게 나오도록 해야한다. 이를 위해서 다음과 같은 normalization이 필요하다.
<script type="math/tex">\hat{x} =\frac{x-E[x]}{\sqrt{Var[x] + \epsilon}}</script>
주목할 점은 여기서 나오는 $E[x], Var[x]$는 모두 mini-batch에서 얻은 것이 아니라 <em>population</em>에서 얻은 것이다. 위의 식에서도 mean=0, var=1로 유지된다. $Var[x]$는 sample variance $\sigma_b^2$로 부터 다음과 같이 구한다.
<script type="math/tex">Var[x] = \frac{m}{m-1}\cdot E_B[\sigma_B^2]</script>
training과정에서 moving average를 사용하는 것과 다르게 inference과정에서는 고정된 mean과 variance를 통해서 deterministic한 output을 도출한다. 이는 각 layer마다 linear transformation을 한 것으로 해석할 수 있다. 알고리즘은 아래와 같다.</p>

<p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/Algorithm2.png" alt="" title="Algorithm2"></p>

<h3 id="batch-normalized-convolutional-networks">
<a class="anchor" href="#batch-normalized-convolutional-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch-Normalized Convolutional Networks</h3>

<p>Affine transformation에 어떻게 적용될 수 있는지 살펴보자.</p>

<blockquote>
  <p>affine transformation</p>

  <p>선형변환에 평행이동이 더해진 개념이다.</p>
</blockquote>

<script type="math/tex; mode=display">z = g(Wu + b)</script>

<p>where $g(\cdot)$ is non-linear such as sigmoid or Relu.</p>

<p>위의 수식은 inputs $u$에 대해서 $x=Wu + b$에 직접 Batch normalization을 적용할 수 있다. 이런 생각을 할 수 있다. input $u$에 batch normalization을 적용 할 수 있지 않을까?  하지만 문제가 있다. $u$는 결국 다른 non-linearity layer의 output이기 때문에 distribution이 training 과정마다 바뀐다. 그리고 첫번째 두번째에 제한을 한다고 해도 결국 covariate shift는 제거할 수 없다.</p>

<p>반면에, $Wu +b$는 symmetric, non-sparse distribution을 가진다. 따라서 이를 normalizing한다면 더 안정적인 결과를 얻을 수 있다.</p>

<p>위에서 언급했듯이 bias $b$는 backpropagation과정에서 무시될 수 있는데 이런 문제를 해결위해서 아래와 같이 위의 수식을 바꿀 수 있다.
<script type="math/tex">z=g(BN(Wu))</script>
여기서 $BN$은 $Wu$의 각 dimention마다 독립적으로 적용되며, $\gamma^{(k)}, \beta^{(k)}$를 얻을 수  있다. ($k$는 각 dimension index를 의미한다.)</p>

<p>그렇다면, Convolution layer에서는 어떻게 적용될 지 살펴보자. 아래는 Convolution filter의 작동방법이다.</p>

<p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/convolution.gif" alt="" title="Convolution"></p>

<p>CNN의 특징중 하나는 ‘local connected’라는 것이다.</p>

<blockquote>
  <p>local connected</p>

  <p>국지적인 정보의 집합을 이용하는 것을 의미한다. 즉 위의 그림처럼 filter안에 있는 정보끼리만 영향을 주며 filter가 이동할 때 이전 filter의 영향을 받지 않는다.</p>
</blockquote>

<p>Convolution layer에서는 BN transformation이 다음과 같이 적용된다.</p>

<ul>
  <li>
    <p>추가적인 normalization이 필요하다. 이를 통해서 같은 feature map상에서 서로 다른 위치에 있는 구성요소를 공통적으로 normalize할 수 있다.</p>
  </li>
  <li>
    <p>이를 위해서 mini-batch상에 모든 activation을 location에 대해서 normalize를 진행한다. (jointly)</p>
  </li>
  <li>
    <p>아래의 Alg.1에서 $B$를 한 feature map에서의 각 location에 대한 activation value로 설정한다.(in mini-batch). mini-batch $B$의 크기는 feature map크기가 $p \times q$라고 가정하고 $m\cdot pq$가 된다. 따라서 $\gamma^{(k)}, \beta^{(k)}$는 activation마다가 아니라 feature map마다 구하게 된다.</p>
  </li>
  <li>
    <p><img src="/FastPages/images/2019-07-08-Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC/Algorithm1" alt="" title="Algorithm1"></p>
  </li>
</ul>

<h3 id="batch-normalization-enables-higher">
<a class="anchor" href="#batch-normalization-enables-higher" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization enables higher</h3>
<p>Batch Normalization을 적용하면 더 높은 learning rate를 사용할 수 있으며, 이는 모델의 수렴속도를 높여준다.</p>

<p>일반적으로 parameter scale이 높으면, model explosion현상이 발생한다. 하지만, batch normalization을 사용하면 parameter scale의 영향을 받지 않는다. 또한 큰 parameter scale에도 smaller gradient를 가진다.
<script type="math/tex">BN(Wu) = BN((aW)U)</script></p>

<script type="math/tex; mode=display">\frac{\partial BN((aW)u)}{\partial u} =\frac{\partial BN(Wu)}{\partial u}</script>

<script type="math/tex; mode=display">\frac{\partial BN((aW)u)}{\partial aW} =\frac{1}{a}\cdot\frac{\partial BN(Wu)}{\partial aW}</script>

<p>또한, 해당논문에서는 BN이 layer jacobians가 1에 가까운 singular value를 가진다는 것을 발견했다. (이는 train할 때 유용한 특성이다.)</p>

<ul>
  <li>
    <p>normalized vector: $\hat{z} =F(\hat{x})$</p>
  </li>
  <li>
    <p>가정: $\hat{x}, \hat{z}$는 uncorrelated되어 있으며, gaussian을 따른다. 또한, 함수 $F(\hat{x}) \approx J \hat{x}$는 linear transformation이다.</p>
  </li>
  <li>
    <p>$\hat{x}, \hat{z}$은 다음과 같은 covariance를 가진다.
<script type="math/tex">I = Cov[\hat{z}] = JCov[\hat{x}]J^T=JJ^T</script>
따라서, $JJ =I$이고 singular value는 1이다. 이는 gradient magnitude를 보존하는 역할을 한다.</p>

    <p>사실 real-world에서는 위의 가정이 사실이라고 하기 힘들지만 그래도 BN의 역할을 알 수 있다.</p>
  </li>
</ul>

<h2 id="batch-normalization-regularizes-the-model">
<a class="anchor" href="#batch-normalization-regularizes-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Normalization regularizes the model</h2>

<p>batch normalization은 Drop-out처럼 regularization효과가 있다.</p>

<p>Reference</p>

<ul>
  <li>https://arxiv.org/abs/1502.03167</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="RRoundTable/FastPages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/FastPages/interview/deeplearning/2019/07/08/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">fastpages</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">fastpages</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li><li><a href="https://www.twitter.com/fastdotai"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">fastdotai</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
