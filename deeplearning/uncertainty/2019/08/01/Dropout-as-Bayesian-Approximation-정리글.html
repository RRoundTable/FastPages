<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Dropout as Bayesian Approximation 정리글 | RoundTable</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Dropout as Bayesian Approximation 정리글" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Problem" />
<meta property="og:description" content="Problem" />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:site_name" content="RoundTable" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-01T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html"},"description":"Problem","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html","headline":"Dropout as Bayesian Approximation 정리글","dateModified":"2019-08-01T00:00:00-05:00","datePublished":"2019-08-01T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="RoundTable" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">RoundTable</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/">Posts</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Dropout as Bayesian Approximation 정리글</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-08-01T00:00:00-05:00" itemprop="datePublished">
        Aug 1, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/FastPages/categories/#deeplearning">deeplearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/FastPages/categories/#uncertainty">uncertainty</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#problem">Problem</a></li>
<li class="toc-entry toc-h2"><a href="#related-research">Related Research</a></li>
<li class="toc-entry toc-h2"><a href="#dropout-as-a-bayesian-approximation">Dropout as a Bayesian Approximation</a>
<ul>
<li class="toc-entry toc-h4"><a href="#dropout-objective는-approximation-distribution과-deep-gaussian-process의-posterior간의-kl-divergence를-감소시킨다">Dropout objective는 approximation distribution과 deep gaussian process의 posterior간의 Kl-divergence를 감소시킨다.</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#obtaining-model-uncertainty">Obtaining Model Uncertainty</a></li>
<li class="toc-entry toc-h2"><a href="#example-code-image-segmentaton">Example code: image segmentaton</a>
<ul>
<li class="toc-entry toc-h4"><a href="#reference">Reference</a></li>
</ul>
</li>
</ul><h2 id="problem">
<a class="anchor" href="#problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem</h2>

<p>일반적으로 Bayesain model은 model의 uncertainty를 측정할 수 있다는 장점이 있지만, computation cost가 너무 커서 사용하기 힘들다는 문제를 가지고 있다. 이런 문제점을 해결하기 위해서 이 논문에서는 Dropout을 사용한 딥러닝 모델이 결국은 gaussian porcess에서의 bayesain inference를 근사한 것이라는 증명을 할 것이다.</p>

<h2 id="related-research">
<a class="anchor" href="#related-research" aria-hidden="true"><span class="octicon octicon-link"></span></a>Related Research</h2>

<ul>
  <li>
    <p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">Bayesian learning for neural networks</a></p>

    <p>infinite-wide neural network에 distribution을 가정하면, 결국 gaussian process를 approxiation하는 것이다. 하지만 finite-wide neural network에서는 증명되지 않았다.</p>

    <p>추가적으로 finite-wide neural network상에서 연구되었다. BNN 조건에서 overfitting문제에 견고했지만, computation cost 높다는 문제가 있었다.</p>
  </li>
  <li>
    <p><a href="http://www.cs.toronto.edu/~fritz/absps/colt93.pdf">variational inference</a></p>

    <p>BNN에서 variational inference가 적용되었다. 하지만, 부분적인 성과만 있었다.</p>
  </li>
  <li>
    <p><a href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">sampling-based variational inference</a></p>

    <p>위의 VI를 개선하기 위해서 sampling 기반의 방법론이 등장하였다. 이 방법론은 dropout만큼 성공적이였으나 computation cost가 매우 높았다. uncertainty를 측정하기 위해서 parameter가 기존의 방법론 대비 두 배가 필요했다. 이는 분포를 정의하기 위해서 평균값과 분산값을 정의해야 되기 때문이다. 게다가 수렴하는 시간도 오래걸렸으며, 기존의 방법론 대비 효과적이지도 않았다.</p>
  </li>
</ul>

<h2 id="dropout-as-a-bayesian-approximation">
<a class="anchor" href="#dropout-as-a-bayesian-approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout as a Bayesian Approximation</h2>

<p>이 section에서는 Dropout이 결국에는 Deep gaussian process를 근사한다는 것을 수학적으로 증명할 것이다. 특히, 어떠한 가정도 없이 증명할 수 있으며, 어떤 network에도 적용가능하다는 장점을 가지고 있다.</p>

<h4 id="dropout-objective는-approximation-distribution과-deep-gaussian-process의-posterior간의-kl-divergence를-감소시킨다">
<a class="anchor" href="#dropout-objective%EB%8A%94-approximation-distribution%EA%B3%BC-deep-gaussian-process%EC%9D%98-posterior%EA%B0%84%EC%9D%98-kl-divergence%EB%A5%BC-%EA%B0%90%EC%86%8C%EC%8B%9C%ED%82%A8%EB%8B%A4" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout objective는 approximation distribution과 deep gaussian process의 posterior간의 Kl-divergence를 감소시킨다.</h4>

<p>우선 Dropout objective를 확인해보자. ( add regularization)</p>

<script type="math/tex; mode=display">\mathcal{L}_{dropout} = \frac{1}{N}\sum_{i=1}^N E(y_i, \hat{y}_i) + \lambda\sum_{i=1}^N (\rVert W_i \rVert_2 ^ 2+ \rVert b \rVert_2 ^ 2)</script>

<p>dropout은 결국 모든 input point와 각 layer의 모든 network에 binary distribution을 적용한 것으로 해석할 수있다. 참고로 output에는 적용하지 않는다. 이는 아래의 이미지 처럼 적용될 수 있다.</p>

<p><img src="/FastPages/images/2019-08-01-Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80/dropout.png" alt="" title="Dropout"></p>

<p>GP 모델에서 predictive probability는 아래와 같이 전개된다. ($x^*$ is unseen)</p>

<script type="math/tex; mode=display">p(y\mid x^*, X, Y) = \int p(y\mid x^*, w) p(w\mid X, Y) dw</script>

<script type="math/tex; mode=display">p(y\mid x, w) = \mathcal{N}(y; \hat{y}(x, w), \tau^{-1}I_D)</script>

<script type="math/tex; mode=display">\hat{y}(x, w= \{ W_1, \cdots, W_L\}) = \sqrt\frac{1}{K_L}W_L\sigma( \cdots \sqrt\frac{1}{K_1}W_2 \sigma(W_1x + m_1) \cdots)</script>

<p>여기서 posterior $p(w\ mid X, Y)$가 untractable한데 이를 해결하기 위해서 variational inference를 사용하며, simple distribution으로 $q(w)$를 가정한다. 이때 $q(w)$는 matrix형태를 가지고 있으며 random하게 0으로 값이 지정된다. (여기서 $K$ 는 matrix dimension을 의미한다. $W_i$ 의 dim은 $K_i * K_{i-1}$)</p>

<script type="math/tex; mode=display">W_i = M_i \cdot diag([z_{i, j}]_{j=1}^{K_i})</script>

<script type="math/tex; mode=display">Z_{i, j} = Bernoulli(p_i)  \ for \ i = 1, \cdots, L, \ j= 1, \cdots, K_{i-1}</script>

<p>여기서 $z_{i, j}$가 0이라면, layer $i - 1$ 의 unit $j$가 drop된다는 것을 의미한다. 위의 이미지에서는 layer에 dropout을 설정한 것을 시각화 한것이라면 위의 수식은 parameter자체에 dropout을 걸었다는 차이가 있다. (그리고 BNN을 적용하기 위해서는 위의 수식처럼 parameter 자체에 dropout을 설정하는 것이 타당하다고 생각한다.)</p>

<p>variational distribution $q(w)$는 highly multi modal한 특징을 가지고 있다. 왜냐하면, 각 layer에 대한 Bernoulli distribution의 output값이 layer의 크기 만큼 나와야 하기 때문이다.</p>

<p>$q(w)$를 바탕으로 objective를 도출하면 아래와 같다. (lower bound: <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational inference</a> 참고)</p>

<script type="math/tex; mode=display">- \int q(w) \log p(Y \mid X, w) dw +KL(q(w) \rVert p(w)).</script>

<ul>
  <li>$p(w)$ 는 prior distribution을 의미한다.</li>
</ul>

<p>첫번째 term $- \int q(w) \log p(Y\mid X, w) dw$은 아래처럼 바꿀 수 있다.</p>

<script type="math/tex; mode=display">- \int q(w) \log p(Y\mid X, w) dw \approx- \sum_{n=1} ^N \int q(w) \log p(y_n \mid x_n, w) dw</script>

<p>두번째 term $ KL(q(w) \rVert p(w))$에서는 아래와 같은 식을 얻을 수 있다.</p>

<script type="math/tex; mode=display">\sum_{i=1}^ L(\frac{p_il^2}{2}\rVert M_i\rVert_2^2 + \frac{l^2}{2}\rVert m_i \rVert_2^2)</script>

<ul>
  <li>$p_i$ bernoulli 분포의 확률값</li>
  <li>$l$ 은 prior length scale 값: appendix section 4.2
    <ul>
      <li>prior distribution에 대한 가정</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>appendix section 4.2
<script type="math/tex">KL(q(w) \rVert p(w)) \approx \sum_{i=1}^L \frac{p_i}{2}(u_i^Tu_i + tr(\Sigma_i) - K(1 + \log2\pi) - \log\rvert\Sigma_i \rvert - C)</script></p>

</blockquote>

<p>model precision $\tau$를 고려하면, 아래와 같이 scale한 값이 도출된다.</p>

<script type="math/tex; mode=display">\mathcal{L}_{GP-MC} \propto \frac {1}{N}\sum_{i=1}^N\frac{- \log p (y_n \mid  x_n, \hat{w_n})}{\tau} + \sum_{i=1}^{L}(\frac{p_i l^2}{2\tau N}\rVert M_i\rVert_2^2 + \frac{l^2}{2\tau N}\rVert m_i \rVert_2^2)</script>

<p>여기서 $\tau$와 length-scale $l$은 hyperparameter이다. lengh-scale은 function frequency를 가정하는 것으로 만약 $l$을 강하게 준다면, regularization 효과는 더 강해진다.</p>

<script type="math/tex; mode=display">\lambda_1 = \frac{l^2 p_1}{2N\tau}</script>

<script type="math/tex; mode=display">\tau = \frac{l^2 p_1}{2N \lambda_1}</script>

<ul>
  <li>short length scale $l$ (high frequency data) + high precision $\tau$(small observation noise) result in small weight-decay $\lambda$ : 모델이 데이터에 더 잘 적합하게 된다.</li>
  <li>long length scale $l$ (low frequency data) + low precision $\tau$ (large observation noise) result in large weight-decay</li>
</ul>

<h2 id="obtaining-model-uncertainty">
<a class="anchor" href="#obtaining-model-uncertainty" aria-hidden="true"><span class="octicon octicon-link"></span></a>Obtaining Model Uncertainty</h2>

<p>predictive distributioin은 아래와 같이 주어진다.</p>

<script type="math/tex; mode=display">q(y^*\mid x^*) = \int p(y^*\mid x^*, w) q(w) dw</script>

<ul>
  <li>$w = {  W_i}_{i=1} ^ L$</li>
  <li>unseen input data: $x^*$</li>
  <li>prediction from unseen input data: $y^*$</li>
</ul>

<p>dropout을 이용하여, uncertainty를 estimate를 하기 위해서는 bernoulli distribution ${z_1^t, \cdots, z_L^t}_{t=1}^T$를  sampling해주면 된다. 이 식에서는 T번의 sampling을 진행한 것이다.</p>

<p>sampling한 분포를 바탕으로 predictive mean값을 근사할 수 있다. 이를 MC-dropout이라고 부른다.</p>

<script type="math/tex; mode=display">E_{q(y^* \mid x^*)}(y^*) = \frac{1}{T}\sum_{i=1}^T \hat{y}^*(x^*, W_1^t, \cdots, W_L^t)</script>

<p>다음은 raw moment를 근사하는 과정이다.</p>

<script type="math/tex; mode=display">E_{q(y^* \mid x^*)}((y^*) ^T y^*) \approx \tau^{-1}I_D +\frac{1}{T}\sum_{i=1}^T \hat{y}^*(x^*, W_1^t, \cdots, W_L^t) ^ T \hat{y}^*(x^*, W_1^t, \cdots, W_L^t)</script>

<p>predictive variance는 다음과 같이 도출된다.</p>

<script type="math/tex; mode=display">Var_{q(y^* \mid x^*)}(y^*) \approx  \tau^{-1}I_D + \frac{1}{T}\sum_{i=1}^T \hat{y}^*(x^*, W_1^t, \cdots, W_L^t) ^ T \hat{y}^*(x^*, W_1^t, \cdots, W_L^t) -E_{q(y^* \mid x^*)}(y^*) ^T E_{q(y^* \mid x^*)}(y^*)</script>

<p>참고로 $y^<em>$는 row vector를 의미하며 $ \hat{y}^</em>(x^<em>, W_1^t, \cdots, W_L^t) ^ T \hat{y}^</em>(x^*, W_1^t, \cdots, W_L^t)$ 연산은 outer product이다.</p>

<p>weight-decay 값 $\lambda$와 length scale $l$이 주어지면 아래의 식으로 <strong>model precision $\tau$</strong> 를 도출할 수 있다.</p>

<script type="math/tex; mode=display">\tau = \frac{l^2 p_1}{2N \lambda_1}</script>

<p>regression task에서 다음과 같이 predictive log-likelihood를 monte-carlo integration을 통해서 근사할 수 있다. 이를 통해서 model이 mean과 얼마나 일치하는지 uncertainty가 어떤지 알 수 있다.</p>

<p>with $w_t \sim q(w)$
<script type="math/tex">% <![CDATA[
\begin{align}
\log p(y^* \mid x^*, X, Y) &= \log \int p(y^* \mid x^*, w) p(w \midX, Y) dw\\
& \approx \log \int p(y^* \mid x^*, w) q(w) dw \\
&\approx \log\frac{1}{T} \sum_{t=1}^{T}  p(y^* \mid x^*, w_t)
\end{align} %]]></script></p>

<p>At regression task,</p>

<script type="math/tex; mode=display">\log p(y^* \mid x^*, X, Y) \approx \mathrm{logsumexp} ( -\frac{1}{2}\tau\rVert y - \hat{y}\rVert^2) -\log T - \frac{1}{2}2\pi - \frac{1}{2}\log \tau ^{-1}</script>

<ul>
  <li>$y$  : predictive mean</li>
  <li>$\hat{y}$ : sample</li>
</ul>

<p>predictive distribution $q(y^* \mid x^*)$은 highly multi modal이기 때문에 그 특성을 정확히 알 수 없다. 이는 weight element에 bi-modal한 distribution을 설정하였고 이들의 joint distribution은 multi modal이기 때문이다.</p>

<p>하지만, 구현하기 매우 싶다.  dropout을 수정하지 않고 사용하며, samples을 모아서 uncertainty를 측정할 수 있다. 또한 forward pass는 기존의 standard 한 모델과 차이가 나지 않는다.</p>

<h2 id="example-code-image-segmentaton">
<a class="anchor" href="#example-code-image-segmentaton" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example code: image segmentaton</h2>

<p>아래는 test과정에서 predictive mean를 구하는 method의 예시이다. 주목할 점은 dropout을 낀채로 sampling을 진행해야 한다는 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_epistemic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">test_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="s">"""Epistemic model Test
    Please turn on Dropout!
    model: pytorch model
    test_loader: test data loader
    crieterion: loss_fucntion
    Return
        test_loss, test_error
    """</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># train mode: turn on dropout
</span>    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">test_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_trials</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span> <span class="c1"># sampling
</span>            <span class="n">outputs</span> <span class="o">+=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">/</span> <span class="n">test_trials</span>  <span class="c1"># predictive mean
</span>        <span class="n">pred</span> <span class="o">=</span> <span class="n">get_predictions</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
        <span class="n">test_error</span> <span class="o">+=</span> <span class="n">error</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
    <span class="n">test_error</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_error</span>

</code></pre></div></div>

<p>다음은 predictive variance를 구하는 과정이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_epistemic</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">predictive_mean</span><span class="p">,</span> <span class="n">test_trials</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">img_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">img_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">target_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bchw,bchw-&gt;bhw"</span><span class="p">,</span> <span class="p">[</span><span class="n">predictive_mean</span><span class="p">,</span> <span class="n">predictive_mean</span><span class="p">])</span><span class="o">.</span><span class="n">data</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test_trials</span><span class="p">):</span>
        <span class="n">output_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
            <span class="s">"bchw,bchw-&gt;bhw"</span><span class="p">,</span> <span class="p">[</span><span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">data</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">output_sq</span> <span class="o">-</span> <span class="n">target_sq</span>
    <span class="n">result</span> <span class="o">/=</span> <span class="n">test_trials</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<h4 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h4>

<ul>
  <li>https://arxiv.org/abs/1506.02142</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="RRoundTable/FastPages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/FastPages/deeplearning/uncertainty/2019/08/01/Dropout-as-Bayesian-Approximation-%EC%A0%95%EB%A6%AC%EA%B8%80.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">RoundTable</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">RoundTable</li><li><a class="u-email" href="mailto:ryu071511@gmail.com">ryu071511@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Tech Blog for RoundTable</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
