<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Problem: bottleneck | fastpages</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Problem: bottleneck" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다." />
<meta property="og:description" content="기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다." />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/deeplearning/2019/07/25/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/deeplearning/2019/07/25/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-25T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2019-07-25T00:00:00-05:00","headline":"Problem: bottleneck","mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/deeplearning/2019/07/25/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html"},"description":"기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다.","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/deeplearning/2019/07/25/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html","dateModified":"2019-07-25T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="fastpages" />

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Problem: bottleneck</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-25T00:00:00-05:00" itemprop="datePublished">
        Jul 25, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/FastPages/categories/#deeplearning">deeplearning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>기존의 state-of-the-art object detection network는 region proposal algorithm을 사용하였다. (ex-Fast R-CNN) region proposal algorithm은 이들 network상에서 bottleneck의 역할을 하고 있었다. 즉, region proposal algorithm 때문에 학습 시간 및 알고리즘 수행시간이 지체되고 있는 것을 확인했다.</p>

<p>이러한 문제를 해결하기 위해서 <strong>Region Proposal Network</strong>를 제안하는데 이는 full-image convolutional feature를 region proposal하는데도 사용하여 cost-free하게 적용될 수 있다.</p>

<blockquote>
  <p>Fast-RCNN: region proposal algorithm</p>
  <h3 id="selective-search">selective search</h3>
  <p>한 이미지 당 cpu기준 약 2초의 시간이 걸린다.
<img src="https://i.imgur.com/DPEJcwI.png" style="width:50%;" /></p>

  <ul>
    <li>reference: https://donghwa-kim.github.io/SelectiveSearch.html</li>
  </ul>
</blockquote>

<h1 id="faster-rcnn">Faster-RCNN</h1>
<p><img src="https://curt-park.github.io/images/faster_rcnn/Figure2.png" /></p>

<p>Faster-RCNN은 두 가지 모듈로 구성된다.</p>

<ol>
  <li>Deep fully convolutional network  that proposes regions</li>
  <li>Fast R-CNN detector(classfier): 1에서 추출된 regioin을 사용한다.</li>
</ol>

<p>중요한 점은 1, 2가 진행되는 동안 feature를 공유한다는 것이다. (cost-free)</p>

<h2 id="region-proposal-networks">Region Proposal Networks</h2>

<p>어떤 사이즈의 이미지가 들어와도 직사각형의 object proposal을 output으로 가진다.</p>

<blockquote>
  <p>fully convolutioinal network</p>

  <p><img src="https://miro.medium.com/max/770/1*BYAi0HtzaJzqAHhAareTNg.png" style="width: 50%;" /></p>
</blockquote>

<p>이  논문의 목표는 detector의 computation과 proposal의 computation을 공유하는 것이기 때문에 위와 같은 convolution layers를 공유한다고 가정하였다.</p>

<ul>
  <li>ZF: 5 sharable convolutional layers</li>
  <li>VGG-16: 13 sharable convolutional layers</li>
</ul>

<p>이렇게 공유된 feature들은 두 가지 용도로 사용하게 된다.  ($n \times n$의 spatial window를 사용하여 얻은 feature)</p>

<ul>
  <li>box regression: proposal</li>
  <li>box classification: detector</li>
</ul>

<p>해당 논문에서는 $n = 3$으로 feature를 생성했는데, 이는 일반적인 reception field로는 작은 숫자이다. 이렇게 작게 설정한 이유는 sliding window가 모든 spatial location을 탐색할 수 있게끔 설계한 것이다.</p>

<h3 id="anchors">Anchors</h3>

<p>sliding window의 위치가 변할 때 마다, multiple region proposals이 이루어진다.</p>

<p>한 위치에서 나올수 있는 region proposal의 최대 개수를 $k$라고 한다면,</p>

<ul>
  <li>regression box: $4k$의 outputs, $k$박스의 각 좌표(꼭지점)</li>
  <li>classfier: $2k$의 outputs, object or not</li>
</ul>

<p><img src="https://www.researchgate.net/profile/Max_Ferguson/publication/327392506/figure/fig8/AS:666613162450944@1535944371721/Anchor-Boxes-at-a-certain-position-in-the-feature-map.png" style="width: 70%;" /></p>

<p>위의 그림은 anchor의 scale과 ratio에 변화이다. 해당 논문에서는 위의 그림과 같이 9 종류의 anchor를 사용하였다.  따라서 해당 이미지의 크기가 $W \times H$라면 $W \times H \times k$개의 anchor가 존재한다.</p>

<h4 id="translation-invariant-anchors">Translation-Invariant Anchors</h4>

<blockquote>
  <p>translation-invariant의 특성이란</p>

  <p><img src="https://i.stack.imgur.com/iY5n5.png" style="width: 70%;" /></p>

  <p>물체가 특정 위치에 존재할 때만 탐지되거나 혹은 특정 위치에서는 탐지가 잘 안되는 현상을 줄이는 것</p>

  <ul>
    <li>reference: https://medipixel.github.io/post/anchor-target/?fbclid=IwAR3sCN1gXjcpt0SNcBgCpVsW8Y6jo2u-2MrBkrQGQgy3CSIKkUPPHGt4YY8</li>
  </ul>
</blockquote>

<p>이를 확인하기 위해서 MultiBox method와 비교하게 되는데, 이는 k-means 방법론을 사용하며 800개의 anchor를 생성한다. 하지만 이는 translation invariant가 아니다.</p>

<p>translation invariant의 특성은 model size를 작게 사용할 수 있게 해준다. MultiBox의 경우 $(4 + 1) * 800$-dimensional fully-connected network를 사용하는 반면에, Faster-RCNN은 $(4 + 2) * 9$-dimensional convolutional output layer만 있으면 가능하다. 즉,  $512 * (4 + 2) * 9 \approx 2.8 * 10^4$의 파라미터만으로 해결 할 수 있다. (box: 4, cls: 2) 반면에, MultiBox는 약 $ 1536 * (4 + 1) * 800 \approx 6.1 * 10^6 $의 파라미터가 필요하다. 따라서, overfitting 문제에 있어서 Faster-RCNN이 더 우수할 것으로 기대된다. ($1536=512 * 3$)</p>

<h4 id="multi-scale-anchors-as-regression-references">Multi-Scale Anchors as Regression References</h4>

<p><img src="https://lh4.googleusercontent.com/k2MADd4a8MolgZiH6j6Zj7nDfoSz3MrHaLmtt2VZuzE4Du8O2w85pJDxz4GhAvGt8piSEATGfUa1Ur7JWBufkigp3s2ZivDZ8HRfSaepVDZK4vjCpgZlPy9AfmNgN4QFmgnzCvW1" /></p>

<p>multi-scale prediction에는 두 가지 방법이 있다. 그리고 이 두가지 방법은 종종 혼합하여 사용된다. 해당 논문에서는 cost-efficient한 두 번째 방법을 사용하였다.</p>

<ul>
  <li>image/feature pyramids -Figure 1 (a)
    <ul>
      <li>이미지를 다양한 크기로 resize한 후 feature map은 각 resize된 이미지에서 뽑아낸다.</li>
      <li>효과적이긴 하나, computation 시간이 오래 걸린다.</li>
    </ul>
  </li>
  <li>multiple scale sliding window
    <ul>
      <li>다양한 크기의 window를 sliding하여 feature을 얻는다.</li>
      <li>pyramid of filters</li>
      <li>image/feature pyramids을 사용하지 않게 하기 때문에 scale을 다루는데 추가적인 cost가 발생하지 않는다.(rescale and feature)</li>
    </ul>
  </li>
</ul>

<h4 id="loss-function">Loss Function</h4>

<p>RPN을 학습할 때는 각 anchor에 대해서 binary class label로 학습이 진행된다. (object or not)
positive label이 부여되는 경우는 두 가지이다.</p>

<ol>
  <li>anchor/ anchors with the highest IOU score with ground-truth boxes</li>
  <li>union anchor: ground-truth box와의 IOU 점수가 0.7이상인 경우</li>
</ol>

<p>보통은 2번째 조건으로만 postive sample을 만들 수 있지만, 희귀한 경우에 2번째 조건만으로 찾지 못하는 경우가 있다. 그래서 1번째 조건도 추가한다.</p>

<p>negative label의 경우 IOU점수가 0.3보다 낮은 anchor에 부여된다.(ground-truth) positive 혹은 negative에 속하지 못하는 anchor는 train objective에 영향을 주지 않는다.</p>

<p>아래는 objective이다.</p>

<p><code class="language-plaintext highlighter-rouge">$$
L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}}\sum_iL_{cls}(p_i, p_i^*) + \lambda\frac{1}{N_{reg}}\sum_i p_i^*L_{reg}(t_i, t_i^*)
$$</code></p>

<ul>
  <li>$i$는 anchor의 index를 의미한다.</li>
  <li>$p_i^*$는 ground-truth label을 의미하며 1이 postive이다.</li>
  <li>$t_i^*$는 4차원의 vector로 bounding box를 의미한다. (ground-truth)</li>
  <li><code class="language-plaintext highlighter-rouge">$p_i^*L_{reg}(t_i, t_i^*)$</code>은 positive sample일때만 적용된다는 뜻이다.</li>
</ul>

<p>Bounding box regression은 어떻게 적용되는가? $L_1$ loss가 적용되며 vector가 다음과 같이 변환된다.
<code class="language-plaintext highlighter-rouge">$$
t_x = (x-x_a)/w_a, t_y=(y-y_a)/h_a\\
t_w = \log(w/w_a), t_h=\log(h/h_a) \\
t_x^* =(x^* - x_a)/w_a, t_y^*=(y^*-y_a)/h_a \\
t_w^*= \log(w^*/w_a), t_h^*=\log(h^*/h_a)
$$</code></p>

<ul>
  <li>$x$: predicted box</li>
  <li>$x_a$: anchor box</li>
  <li>$x^*$: ground truth box</li>
</ul>

<p>위의 방법은 기존에 사용되던 Region of Interest methods와 다르다. ROI method에서는 bounding-box regression이 arbitrarily sized ROI에서 뽑힌 feature를 바탕으로 진행되며, 모든 region size에 대해서 weight를 공유한다. 반면에 위의 방법론은 feature map의 spatial size가 3 * 3으로 고정되어 있으며 변하는 size에 다루기 위해서는 다양한 anchor를 사용한다. 이 때, 각 anchor끼리는 weight를 공유하지 않는다.</p>

<p>하지만, 위에서 언급했듯이 이전의 region proposal 방법론은 효율적이지 못하다는 단점이 있다.</p>

<h4 id="training-rpns">Training RPNs</h4>

<p>image-centric sampling 방법론을 사용한다.</p>

<p>각 mini-batch에서 image상에는 많은 positives와 negatives가 존재한다. 이를 그대로 학습시킨다면 일반적으로 negative sample의 수가 많기 때문에 편향될 위험이 있다. 따라서, positive sample과 negative sample을 random하게 1:1로 추출한다음 학습을 진행한다. (p: 128, n:128)</p>

<h2 id="sharing-features-for-rpn-and-fast-r-cnn">Sharing Features for RPN and Fast R-CNN</h2>

<p>이제 detection network를 고려해보자. detection network와 regioin proposal network를 독립적으로 학습시킨다면 이는 서로 다른 방법으로 학습이 될 것으로 기대된다. 따라서 convolution layer를 공유하기 위해서는 새로운 방법이 필요하다.</p>

<ul>
  <li>
    <p>Alternating training</p>

    <p>먼저 RPN을 학습시킨다 다음, RPN에서 나오는 Proposal을 바탕으로 Fast-RCNN을 학습시킨다. 해당 논문에서는 이 방법론을 사용한다.</p>
  </li>
  <li>
    <p>Apporximate joint training:</p>

    <p>Fast-RCNN과 RPN을 하나의 network로 만든다. SGD interations동안 forward pass에서 region proposal을 생성하고 이는 Fast-RCNN detector를 학습시키는 동안 미리 계산되고 고정되어 있다.(not differential) backward pass에서는 RPN loss와 Fast_RCNN loss가 더해져서 total loss로 정의된다. 이는 쉽게 적용될 수 있으나, 이는 proposal box에 대한 gradient값을 무시하게 된다. (approximate한다)</p>

    <p>해당 실험에서는 학습시간을 줄이면서 결과는 유사하게 나오는 것을 확인할 수 있었다. (reduce 25% ~ 50% train time)</p>
  </li>
  <li>
    <p>Non-approximate joint training</p>

    <p>Apporximate joint training과 다르게 box coordinates에 대해서 미분가능하다.</p>

    <p>bbox에 미분가능한 ROI pooling layer가 필요하다.</p>
  </li>
</ul>

<h5 id="4-step-alternating-training">4-Step Alternating Training</h5>

<ol>
  <li>RPN 학습, (initialized with an ImageNet-pre-trained model )</li>
  <li>1에서 학습학 RPN을 바탕으로 regioin proposal 생성 후 Fast-RCNN detector 학습(initialized with an ImageNet-pre-trained model) 이 step에서는 layer를 공유하지 않는다.</li>
  <li>detector network를 RPN initialization에 사용한다. 공유되는 layer는 고정시키고 RPN에만 적용되는  layer만 학습시킨다. 이 step에서 layer를 공유한다.</li>
  <li>공유되는 layer는 고정시키고 Fast-RCNN에만 적용되는 layer를 학습시킨다.</li>
</ol>

<h2 id="implementation-detatils">Implementation Detatils</h2>

<p>image/feature pyramids -Figure 1 (a)과 multi scale sliding window 두 가지 방법 모두 실험해보았다. 하지만 첫번째 방법론은 speed-accuracy trade-off가 좋지 않음을 확인할 수 있었다.</p>

<h4 id="recall-to-iou">Recall to IOU</h4>

<p>일반적으로 전반적인 detection accuracy와 관련있는 metric이다.</p>

<p><img src="https://curt-park.github.io/images/faster_rcnn/Figure4.png" /></p>

<p>위의 이미지를 보면 알 수 있듯이, SS, EB가 더 빠르게 감소하는 recall을 확인할 수 있다.</p>

<blockquote>
  <p>recall이란</p>

  <p>TP / total ground-truth</p>
</blockquote>

  </div><a class="u-url" href="/FastPages/deeplearning/2019/07/25/Faster-RCNN-%EC%A0%95%EB%A6%AC%EA%B8%80.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">fastpages</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">fastpages</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li><li><a href="https://www.twitter.com/fastdotai"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">fastdotai</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
