<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Mixup: Beyond Empirical Risk Minimization | fastpages</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Mixup: Beyond Empirical Risk Minimization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다." />
<meta property="og:description" content="mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다." />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/deeplearning/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/deeplearning/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-21T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2019-07-21T00:00:00-05:00","headline":"Mixup: Beyond Empirical Risk Minimization","mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/deeplearning/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html"},"description":"mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다.","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/deeplearning/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html","dateModified":"2019-07-21T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="fastpages" />

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Mixup: Beyond Empirical Risk Minimization</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-07-21T00:00:00-05:00" itemprop="datePublished">
        Jul 21, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/FastPages/categories/#deeplearning">deeplearning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><strong>mixup은 deep learning model의 memorization문제나 adversarial examples에 민감한 이슈를 해결하기 위해 나온 Data Augmentation기법입니다.</strong></p>

<ul>
  <li>memorization:</li>
</ul>

<p>모델이 학습을 진행할 때, 정답만을 기억하고 내리는 행동. 
즉, 데이터 분포를 학습하는 것이 아니라 해당 데이터가 어떤 라벨에 해당하는지 기억하게 되는 것.
<em>결론적으로는 test distribution에 대한 generalization을 하지 못한다.</em></p>

<ul>
  <li>sensitivity to adversarial examples: adversarial attack에 취약하다.</li>
</ul>

<p><strong>mixup은 network를 data pair간의 convex combination을 이용하여 학습시킵니다. 이는 결과적으로 모델이 training sample간에 simple linear behavior를 하지 않도록 하는 효과가 있습니다.</strong></p>

<ul>
  <li>simple linear behavior between training examples:</li>
</ul>

<h2 id="empirical-risk-minimizationerm-vs-vicinal-risk-minimizationvrm">Empirical Risk Minimization(ERM) VS Vicinal Risk Minimization(VRM)</h2>

<p><strong>[성공적인 neural networks의 두 가지 특징]</strong></p>

<ol>
  <li>
    <p>trained as to minimize their average error over the training data.</p>
  </li>
  <li>
    <p>the size of neural networks scales linearly with the number of training examples.</p>
  </li>
</ol>

<p>하지만, learning thoery에 따르면 ERM의 수렴은 모델의 복잡도가 데이터의 수보다 크지 않을 때 보장된다. 이는 [2]번 조건과 모순되어 보일 수 있으나 overfitting issue를 고려해보면 이해가 된다.</p>

<p><strong>[memorization의 측정]</strong>
adversarial examples에 예측하는 라벨이 크게 변하는 정도가 클수록 memorization이 크다고 평가할 수 있다. 
직관적으로 생각해보면, 사람의 지각으로는 큰 차이가 없는 데이터에 대해서 딥러닝 모델이 서로 다른 예측을 한다면 이는 generalization을 못한 것으로 평가할 수 있다.</p>

<p><strong>[Vicinial Risk Minimization이란]</strong>
data augmentation의 이론적 배경으로 training data와 유사한 주변 데이터를 묘사하는 것이다.
이것이 가능해지면, virtual example(만들어진 데이터)는 training distribution의 support를 확대하는 효과를 가져온다. 
(training distribution중 모호한 부분을 채워주는 것으로 해석)</p>

<p>참고: <a href="https://wikidocs.net/17374">[support of distribution]</a></p>

<h2 id="contribution">Contribution</h2>

<script type="math/tex; mode=display">X_{new} = {\lambda}X_i + (1 - {\lambda})X_j</script>

<script type="math/tex; mode=display">Y_{new} = {\lambda}Y_i + (1 - {\lambda})Y_j</script>

<p>위의 수식대로 data augmentation을 하는 것이 mixup의 전부이다.</p>

<p>아래의 영상은 lambda값에 따라서 mixup 데이터의 pca결과값이 어떻게 변하는지 시각화한 것이다. 아래의 그림들처럼 linear하게 PCA value가 변하는 것을 확인하였는데 이는 VRM 가정이 옳다는 것을 보여준다.</p>

<center>
<img src="https://github.com/RRoundTable/mixup_keras/raw/master/results/sample1_3.gif" style="width: 60%;" />

<img src="https://github.com/RRoundTable/mixup_keras/raw/master/results/sample%5B5%5D_%5B4%5D.gif" style="width: 60%;" />

<img src="https://github.com/RRoundTable/mixup_keras/raw/master/results/sample6_3.gif" style="width: 60%;" />
</center>
<h2 id="from-empirical-risk-minimization-to-mixup">From Empirical Risk Minimization To Mixup</h2>

<p>Supervised learning에서의 task는 결국 random feature vector X와 random target vector Y간의 관계를 설명할 수 있는 함수 f를 찾는 것이다. 함수 f는 joint distribution P(X, Y)를 따른다.
이 때, loss function의 역할은 prediction f(X)와 target Y간의 차이를 나타내며 학습을 진행하면서 average loss를 감소시킨다. 여기서 average loss는 <strong><em>expected risk</em></strong>으로 해석된다.
이를 수식으로 나타내면 아래와 같다.</p>

<script type="math/tex; mode=display">{R}(f) = \int {loss}({f}(x), y)dP(x,y)\ \ [1]</script>

<p>하지만, distribution P는 실제상황에서 알기 힘들다. (intractable distribution)
이를 해결하기 위해서 <strong><em>empricial distribution</em></strong>으로 근사하는 방법론을 이용한다. 이를 수식으로 나타내면 아래와 같다.</p>

<p><code class="language-plaintext highlighter-rouge">$${P}_{\sigma}=\frac{1}{n}\Sigma_{i=1}^{n}\sigma(x={x}_i, y={y}_i) \ \ [2]$$</code></p>

<p><code class="language-plaintext highlighter-rouge">$$\sigma(x={x}_i, y={y}_i) 는\ Dirac\ mass\ centered \ at\ (x_i, y_i)\ 성질을 가지고 있다.   $$</code></p>

<p>[1]수식에 [2]수식을 대입하면 아래와 같이 전개될 수 있다.</p>

<p>참고: <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac mass centered</a>
아래 그림과 같이 굉장히 naive한 가정이다.</p>

<center>

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Dirac_distribution_PDF.svg/325px-Dirac_distribution_PDF.svg.png" style="width:50%;" />

</center>

<p><code class="language-plaintext highlighter-rouge">$$R_{\sigma}=\int loss(f(x), y)dP_{\sigma}(x,y)=\frac{1}{n}\Sigma_{i=1}^{n}loss(f(x_i),y_i)$$</code></p>

<p>위와 같이 ERM은 간단하게 계산될 수 있지만, Memorization이라는 현상을 야기할 수 있다. 
joint distribution P에 어떤 가정을 하느냐에 따라서 결과가 달라질 수 있는데 이 논문에서는 <strong>Vicinal Risk Minimization Principle</strong>을 적용하고 있다.
VRM을 가정하게 되면 P는 다음과 같이 정의할 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">$$P_v(\tilde{x}, \tilde{y})=\frac{1}{n}\Sigma_{i=1}^{n}V(\tilde{x}, \tilde{y}|x_i, y_i) \ \ where \ v\ is \ a \ \ a \ \ vicinity \ distribution$$</code></p>

<p>vicinity distribution은 만들어낸 feature-target pair의 probability를 측정한다. 즉, 얼마나 그럴듯한 데이터인지 판단하는 것이다.</p>

<p><strong>[what is mixup doing?]</strong></p>

<center>

<img src="https://pic4.zhimg.com/v2-eac38dd1863c1d26f547bb91d04fe924_1200x500.jpg" style="width: 70%;" />

</center>

<p>Figure 1 (b) Effect of mixup on a toy problem.
Green: Class 0
Orrange: Class 1
Blue shading indicates P(y = 1| x).</p>

<p>위의 그림을 통해서 할 수 있듯이, mixup은 uncertainty를 측정하는데 더 효과적이다. 
파란색 부분은 해당 데이터 x가 주어졌을 때, Class 1일 확률이다. ERM을 보면 파란색 부분이 Class 1가 가까운 것과 가깝지 않는 것 사이의 차이를 나타내지 못한다.
반면에 mixup은 가까운 부분은 더 짙은 파란색으로 나타내어, uncertainty를 smoother하게 측정할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589051-ef28dc80-abdf-11e9-9650-b8cfca8e59a0.jpg" /></p>

<p>(a) 는 prediction을 나타낸 것이고, (b)는 gradient norm을 나타낸 것이다.
(a)를 보면 mixup으로 학습시킨 것이 더 prediction측면에서 좋은 성능을 보이고 있다.</p>

<p>한편, (b)를 보면 gradient norm이 더 작게 나는 것을 알 수 있는데, 이는 더 안정적인 학습을 보이고 있다는 것을 보여준다.</p>

<p>참고: norm</p>
<ul>
  <li>일반적으로 크기 혹은 길이를 나타낸다.</li>
</ul>

<h2 id="실험결과">실험결과</h2>

<p><strong>[3.1 IMAGENET CLASSIFICATION]</strong>
evaluation과정에서 224 * 224 크기의 중간부분이 소실된 이미지를 test하였다.  실험결과 hyperparameter alpha는 0.1에서 0.4사이의 값이 우수한 성능을 보였으며, alpha값이 이보다 더 크면 underfitting의 부작용을 가져왔다. 또한 mixup은 higer capacities와 longer training run의 효과를 가져왔다. 이는 ERM과 비교했을 때 , epoch 90에서 mixup이 더 큰 성능개선효과를 보였다는 것으로 증명하였다.</p>

<p><strong>[3.2 CIFAR10 AND CIFAR100]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589071-24cdc580-abe0-11e9-8fdd-9575261c1d15.png" /></p>

<p>실험결과: imagenet</p>

<p><strong>[3.3 SPEECH DATA]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589086-50e94680-abe0-11e9-8d39-7ee284126604.png" /></p>

<p><strong>[3.4 MEMORIZATION OF CORRUPTED LABELS]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589115-986fd280-abe0-11e9-805a-f546f7531bc8.png" /></p>

<p>mixup interpolation alpha가 더 커질수록 memorization이 더 어려워진다는 가설을 세웠다.
corruption label을 가지고 학습을 시켰다. 이는 memorizaiton을 검증하는 실험에서 사용된다. (더 찾아보기)
실험결과 test과정에서 large alpha가 dropout(0.7, 0.8)보다 test error를 더 줄일 수 있었으며, real label에 대해서는 낮은 training error를 보이고 noisy label에 대해서는 높은 training error를 보였다. 주목할 점은 dropout과 mixup을 같이 사용했을 때 성능이 가장 좋았다.</p>

<p><strong>[3.5 ROBUSTNESS TO ADVERSARIAL EXAMPLES]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589119-acb3cf80-abe0-11e9-8b5c-12f60a21f108.png" /></p>

<p><strong>[3.6 TABULAR DATA]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589126-c35a2680-abe0-11e9-8b30-21679d86d0e5.png" /></p>

<p><strong>[3.7 STABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS]</strong></p>

<p><img src="https://user-images.githubusercontent.com/27891090/61589135-d5d46000-abe0-11e9-97fa-96bd771c656c.png" /></p>

<p>mixup은 disciriminator의 <strong>gradient regularizer</strong>의 역할을 하기 때문에, 더 안정적인 학습을 할 수 있다.</p>

<p><code class="language-plaintext highlighter-rouge">$$
\max_g\min_d E_{x,  z}\mathcal{l}(d(x), 1) + l(d(g(z)), 0)
$$</code></p>

<p>기존의 GAN objective</p>

<p>아래는 mixup이 적용된 <strong><em>GAN distriminator objective</em></strong>이다.</p>

<p><code class="language-plaintext highlighter-rouge">$$max_gmin_dE_{x, y, \lambda}l(d(\lambda x + (1-\lambda)g(z), \lambda).$$</code></p>

<p><strong>[3.8 ABLATION STUDIES]</strong></p>

<p>이 논문에서는 convex combination을 통한 data augmentation방법론을 제안했지만, 다양한 방법론이 있을 수 있다.
[예시]</p>

<ul>
  <li>feature map을 섞는다.</li>
  <li>같은 클레스의 데이터</li>
  <li>비슷한 거리에 있는 데이터</li>
  <li>다른 클레스의 데이터</li>
</ul>

<p>실험결과 현 논문에서 제시한 mixup이 제일 좋은 성능을 보였다.</p>

  </div><a class="u-url" href="/FastPages/deeplearning/2019/07/21/mixup-%EC%A0%95%EB%A6%AC%EA%B8%80.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">fastpages</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">fastpages</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li><li><a href="https://www.twitter.com/fastdotai"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">fastdotai</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
