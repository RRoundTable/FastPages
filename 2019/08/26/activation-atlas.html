<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/FastPages/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction | RoundTable</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Introduction" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What have these networks learned that allows them to classify images so well?" />
<meta property="og:description" content="What have these networks learned that allows them to classify images so well?" />
<link rel="canonical" href="https://rroundtable.github.io/FastPages/2019/08/26/activation-atlas.html" />
<meta property="og:url" content="https://rroundtable.github.io/FastPages/2019/08/26/activation-atlas.html" />
<meta property="og:site_name" content="RoundTable" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-26T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://rroundtable.github.io/FastPages/2019/08/26/activation-atlas.html"},"description":"What have these networks learned that allows them to classify images so well?","@type":"BlogPosting","url":"https://rroundtable.github.io/FastPages/2019/08/26/activation-atlas.html","headline":"Introduction","dateModified":"2019-08-26T00:00:00-05:00","datePublished":"2019-08-26T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/FastPages/assets/main.css">
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://rroundtable.github.io/FastPages/feed.xml" title="RoundTable" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/FastPages/">RoundTable</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FastPages/about/">About Me</a><a class="page-link" href="/FastPages/">Posts</a><a class="page-link" href="/FastPages/search/">Search</a><a class="page-link" href="/FastPages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-08-26T00:00:00-05:00" itemprop="datePublished">
        Aug 26, 2019
      </time>
    •<span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>What have these networks learned that allows them to classify images so well?</p>

<p>네트워크가 classification을 잘하는 이유를 찾기 위해서 다음과 같은 시도를 하였다.</p>

<p>기본적으로 네트워크를 시각적으로 분석할려고 노력했다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63659574-0c5b5700-c7ed-11e9-9f00-d9a8b5274b1b.png" style="width: 70%" /></p>

<ul>
  <li>
    <p><a href="https://distill.pub/2017/feature-visualization/">individual neurons</a></p>

    <p>뉴런들을 독립적으로 시각화</p>
  </li>
  <li>
    <p><a href="https://distill.pub/2017/feature-visualization/#interaction">Interaction between Neurons</a></p>

    <p>뉴런은 독립적으로 움직이는 것이 아니기 때문에 simple feature combination을 시각화함. 이런 시도는 문제점을 가지고 있었다.</p>

    <p>예를 들어, 수 많은 combination중 어떤 combination을 살펴봐야하는지 어떻게 알 수 있는가?</p>
  </li>
  <li>
    <p><a href="https://distill.pub/2018/building-blocks/#ActivationGridSingle">spatial activation</a></p>

    <p>위의 질문에 대한 답은 activation을 시각화하는 것에 있다. 특정 input tensor에 대해서 activation되는 뉴런들의 combination을 시각화하는 것이다.</p>
  </li>
</ul>

<p>위의 접근방법은 hidden layer를 다루는데 탁월하지만, 치명적인 결함이 있다. 하나의 input에 대해서만 시각화한다는 점이다. 이는 각 network의 전반적인 시각적 분석을 하기 힘들다는 뜻이다.</p>

<p>해당 논문은 이런 문제의식을 가지고 “Activation Atlas”를 기획하였다.</p>

<p>이런 global view를 얻기 위한 방법으로는 다음과 같은 방법이 있다.</p>

<ul>
  <li>
    <p><a href="https://cs.stanford.edu/people/karpathy/cnnembed/">CNN code visualization</a></p>

    <p><img src="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k_icon.jpg" style="width: 40%" /></p>

    <p>t-SNE 기반의 시각화 방법이다. 간략히 설명하면, 각 input tensor 혹은 activation value에 대해서 t-SNE로 mapping 시킬 좌표를 구하고 해당 좌표에 위와 같이 이미지를 시각화 하는 것이다.</p>
  </li>
</ul>

<p>activation atlas의 경우 위와 t-SNE와 유사한 방법을 이용하였으나, 주된 차이는 input tensor가 아닌 각 feature를 시각화하는 것에 있다. 각 feature를 위와 같이 시각화하면 feature간의 관계를 파악할 수 있다는 장점이 있다.</p>

<p>activation atlas는</p>

<ul>
  <li>각 feature의 관계를 잘 파악할 수 있다는 강점을 가지고 있으나,</li>
  <li>data distribution에 영향을 받는다는 단점 또한 가지고 있다.</li>
</ul>

<h2 id="looking-at-single-images">Looking at single images</h2>

<p>activation atlas를 살펴보기전에 activation vector를 시각화하는 <a href="https://distill.pub/2018/building-blocks/">spatial activation</a>부터 살펴볼 것이다. 사용할 모델은 InceptionV1이이며, 시각화 과정은 다음과 같다.</p>

<ol>
  <li>
    <p>feed the image into InceptionV1</p>
  </li>
  <li>
    <p>collect activations</p>

    <p>여기서 수집한 activation은 단순한 vector이기 때문에 인간의 눈으로 해석하기 힘들다. 여기서 feature visualization이 필요하다. 단순하게 생각하면, <a href="https://distill.pub/2017/feature-visualization/">feature visualization</a>은 model이 생각하는 특정 activation vector를 생성하는 image를 시각화한 것이다. 일반적으로 image를 activation vector로 바꾸는 흐름과 다르게 activation atlas에서는 activation vector에서 image를 재현하는 흐름으로 간다고 생각하면 된다.</p>
  </li>
</ol>

<p>InceptionV1은 convolution layers로 이루어져 있으므로, 각 layer마다 복수의 activation vector가 존재한다. (Filter의 수만큼) 또한 아래의 이미지 처럼 하나의 뉴런이 각 patch를 이동하면서 activation vector를 생성한다. (Parameter-sharing)</p>

<p><img src="https://taewanmerepo.github.io/2018/01/cnn/filter.jpg" style="width: 30%" /></p>

<p>그러므로, network에 input image를 넣으면 하나의 뉴런은 많은 수의 evaluation을 받는다. 우리는 이를 각 뉴런이 각 patch에 대해서 얼마나 활성화됐는지 평가할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63660761-ed5fc380-c7f2-11e9-8481-d680187ecfcf.png" style="width: 70%" /></p>

<p><img src="https://distill.pub/2019/activation-atlas/assets/images/dogcat-grid.jpg" /></p>

<h2 id="aggregating-multiple-images">Aggregating Multiple Images</h2>

<p>위의 방법론은 single image에 대해서만 접근한 것이다. 하지만, global view를 얻고 싶다면 어떻게 해야할까?</p>

<p>모든 이미지에 대해서 위의 방법론을 적용할 수도 있겠으나, 그러한 방법은 scale-up할 수 없으며 인간의 두뇌는 구조적인 정리없이 수많은 이미지를 모두 인지할 수 없다.</p>

<p>우선, 먼저 수많은 이미지로부터 activation value를 수집해보자. 이는 위와 동일한 방법을 반복하면 된다. 수집한 activation은 위와 동일하게 feature visualization을 적용한다.</p>

<p>이렇게 수집된 vector는 high-dimension(512 dim)의 성격을 가진다. 이를 dimensionality reduction방법론을 적용해서 2차원으로 mapping 하면 아래의 이미지처럼 나타나게 된다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63661257-b8547080-c7f4-11e9-9ee5-972d32716965.png" style="width: 60%" /></p>

<p>Feature visualization을 적용할 때, regularization을 사용하였다.(ex: <a href="https://distill.pub/2017/feature-visualization/#regularizer-playground-robust">transformation robustness</a>)</p>

<p>다른 objective를 사용하기도 하였다. activation space $v$ 를 시각화 하기 위해서, point $x, y$의 activation vector $h_{x,y}$ 를 dot product하였다. ( $h_{x, y} \cdot v$)</p>

<p>해당 논문은 dot product에 cosine similarity를 곱하여 anlge을 강조하는 것이 효과적이라는 것을 발견하였다.
<script type="math/tex">\frac{(h_{x,y} .v)^{n+1}}{(\rVert h_{x,y}  \rVert  \cdot \rVert v \rVert)^{n+1}}</script></p>

<blockquote>
  <p>We also find that whitening the activation space to unstretch it can help improve feature visualization. ???</p>
</blockquote>

<blockquote>
  <p>cosine similarity
<script type="math/tex">similarity = \cos(\theta)= \frac{A\cdot B}{\rVert A \rVert \rVert B \rVert}</script>
<img src="https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png" style="width: 50%" /></p>
</blockquote>

<p>각 activation vector마다 attribution vector를 구할 수 있다. attribution vector란, 각 calss에 대한 항목이 있으며 각 class의 logit에 영향을 받은 activation vector의 값을 근사한다. attribution vector는 주변 contex에 영향을 받는다.
<script type="math/tex">h_{x, y} \cdot \nabla_{h_{x_y}}logit_c</script></p>

<ul>
  <li>Class c logit: $logit_c$</li>
  <li>해당 수식은 뉴런이 logit에 미치는 영향을 측정하는 것</li>
  <li>GradCam과 유사하지만, gradient spatial averaging을 사용하지 않고 gradient의 noise를 continuous relaxtion을 통해서 감소 시켰다.</li>
  <li>Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/building-blocks/AttrSpatial.ipynb</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/27891090/63746516-f7132500-c8df-11e9-9bd2-67cdc3e2892e.png" style="width: 50%" /></p>

<p>위에서 보이는 이미지는 오른쪽의 feature space에 상단 좌측에 위치한 average attribution을 시각화 한 것이다. 위의 이미지는 모두 조금씩 다르지만, 비슷한 류의 동물의 형상을 하고 있다. 특히, 눈, 털, 코 등의 특징을 잡아내고 있다. 주의 할 점은 앞단의 레이어에서 실행하면, 상당히 혼란스러울수 있다는 점이다. (앞단의 레이어에서는 위와 같은 특징을 못잡을 수도 있다.)</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63746563-1611b700-c8e0-11e9-9e52-3b39f3d1c07d.png" style="width: 50%" /></p>

<p>위의 이미지는 좌측 하단의 위치한 average attribution vector이다. 위의 이미지와 다르게 바다 해변의 형상을 가지고 있다.</p>

<blockquote>
  <p>seashore class를 확인하기 위한 activation이 starfish나 sealion과 같은 class를 확인하는데도 쓰이는 것을 확인할 수 있었다.</p>
</blockquote>

<p>위의 두 사례를 보았을 때, 해당 activation atlas가 유의미한 2차원 좌표(semantic)를 가지고 있음을 확인할 수 있다.</p>

<h2 id="looking-at-multiple-layers">Looking at Multiple Layers</h2>

<p>위에서는 하나의 레이어에서 다른 object가 어떻게 시각화되는지 확인하였다면, 이번 세션에서는 유사한 object에 대해서 서로 다른 layer에서 어떻게 나타타는지 알아볼 것이다.</p>

<p>사용할 레이어는 다음과 같다.(강조된 부분)</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63747602-7ace1100-c8e2-11e9-8783-dd9222cb1008.png" style="width: 50%" /></p>

<p><img src="https://user-images.githubusercontent.com/27891090/63747623-87526980-c8e2-11e9-8a3d-15cbc80a8812.png" style="width: 70%" /></p>

<p>위의 이미지는 cabbage class를 시각화한 것이다. 왼쪽에서 오른쪽으로 갈 수록 더 cabbage처럼 구체적이고 복잡해지는 것을 확인할 수 있다. 이는 해당연구에서 기대했던 바인데 이유는 다음과 같다.</p>

<ul>
  <li>뒷 단의 레이어일수록 receptive field가 크기 때문</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/27891090/63747659-9a653980-c8e2-11e9-9b0b-775754cfeb6e.png" style="width: 70%" /></p>

<p>위의 이미지는 sand와 water 그리고 sandbar의 이미지의 activation value를 나타낸 것이다. sandbar를 보면, 앞의 두 이미지를 합친 것과 유사해 보인다.</p>

<h2 id="focusing-on-a-single-classification">Focusing on a Single Classification</h2>

<p>이제부터는 network가 classification하는 것에 대해서 살펴볼 차례이다.</p>

<p>예를 들어서, network가 어떤 과정을 거쳐서 ‘fireboat’라는 class로 결정하는지 살펴볼 것이다.</p>

<p><img src="https://distill.pub/2019/activation-atlas/assets/images/fireboat-01.jpg" style="width: 30%" /></p>

<p>먼저 last layer(mixed5b)를 살펴볼 것이다. 뚜렷하게 보이는 부분일수록 ‘fireboat’로 결정하는데 큰 기여를 한 activation이다. classification 전의 layer이기 때문에 ‘fireboat’와 매우 유사한 이미지들이 진하게 보인다는 것을 확인 할 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63762216-6b0ff600-c8fd-11e9-9b99-958d99722393.png" style="width: 70%" /></p>

<p>아래의 이미지는 mixed4d의 이미지로 여러가지 부분의 조합으로 ‘fireboat’로 인식하고 있음을 확인할 수 있다. 각 부분들은 fireboat와 유사해 보이지 않지만, 위의 fireboat사진을 보면 이렇게 인식하는 이유를 이해할 수 있다.</p>

<p>fireboat를 보면 창문 + 기중기 + 물로 이루어져 있음을 알 수 있다. 해당 부분도 물, 기중기, 창문들로 이루어져 있다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63762688-38b2c880-c8fe-11e9-814c-cefa31eff0bb.png" style="width: 70%" /></p>

<p>이러한 특성은 ‘fireboat’와 ‘streetcar’와 비교해보면 잘 알 수 있다. (조금 유사하지만 다른 object)</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63763195-31d88580-c8ff-11e9-8325-5e1aa3b42f1d.png" style="width: 70%" /></p>

<p>해당 이미지를 보면 streetcar는 기중기나 물에서는 약한 activation을 가지고 있으나 창문과 집에 대한 activation에서는 매우 강한 activation을 가진다. 반대로 fireboat는 물과 기중기, 창문에서는 강한 activation을 가지지만 집에 대한 activation에서는 약한 activation을 가지고 있음을 확인할 수 있다.</p>

<h2 id="further-isolating-classes">Further Isolating Classes</h2>

<p>특정 class에 기여하는 activation만을 확인하고 싶다면, 다른 activation을 완전히 제외할 수 있다. 이를 class-specific activation이라고 한다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63764502-ebd0f100-c901-11e9-8f8e-780a55ac0f1d.png" style="width: 50%" /></p>

<ul>
  <li>
    <p>스노쿨링 이미지</p>
  </li>
  <li>
    <p>Code: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/activation-atlas/class-activation-atlas.ipynb</p>
  </li>
</ul>

<p>class activation atlas는 특정 class에 대해서 어떤 detector가 더 많은 기여를 했는지 명확하게 보여준다. 위의 스노쿨링 예시에서 강한 attribution만 보여주는 것이 아니라, strength가 약하더라도 해당 class에 전반적인 영향을 끼친 attribution도 보여준다. 특정 경우 우리가 보고 싶어하는 object와 매우 강하게 상관관계가 있는 object가 있다. (스노쿨러 - 물고기) 물고기는 우리가 보고 싶어하는 부분과 다른 부분이다. 따라서 적절한 filtering 방법이 필요하다</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63765574-4c612d80-c904-11e9-924f-e68fd1c39712.png" style="width: 100%" /></p>

<p>위의 이미지 다른 필터링을 적용한 것이다. 위의 설명을 참고 바란다.</p>

<p>이제는 유사한 두 클레스를 비교해볼 것이다. (magnitude 기준으로)</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63766039-4e77bc00-c905-11e9-8e8b-21c7a8e4a172.png" /></p>

<p>위의 이미지를 보면 두 클레스를 구분하기 힘들것이다. 아래의 이미지를 보면 도움이 될 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63766282-d067e500-c905-11e9-89f8-8bb8a300ee0c.png" style="width: 70%" /></p>

<blockquote>
  <p>To help make the comparison easier, we can combine the two views into one. We’ll plot the difference between the attributions of the “snorkel” and “scuba diver” horizontally, and use t-SNE  to cluster similar activations vertically.</p>
</blockquote>

<p>위에 주목할 점은 locomotive(기관차)가 스쿠버 다이버와 연관이 깊게 나온다는 것이다. 이를 바탕으로 다음과 같은 실험을 진행하였다.</p>

<p>스노쿨링 이미지에 조금씩 기관차 이미지를 사이즈 업하여 더한 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63766479-318fb880-c906-11e9-9562-d3bac82703a8.png" style="width: 70%" /></p>

<p>해당 이미지를 보면 조금 더 하면 스쿠버 다이버의 softmax값이 올라가나 일정수준이 넘으면 기관차로 인식함을 알 수 있다. 아마도 기관차의 스팀이 그런역할을 한 것으로 보이며 이와 같은 feature를 multi-use feature라고 칭한다.(시각적으로 유사해 보여도 서로 다른 시각적으로 다른 class에 반응)</p>

<p>위와 같은 실험을 attack의 개념으로 1000여번을 진행했다.</p>

<p><img src="https://user-images.githubusercontent.com/27891090/63767051-7bc56980-c907-11e9-96f4-8c7b2f55a2cc.png" style="width: 60%" /></p>

<p><img src="https://user-images.githubusercontent.com/27891090/63767073-867ffe80-c907-11e9-8592-8e2e0f1cf29f.png" style="width: 60%" /></p>

<p><img src="https://user-images.githubusercontent.com/27891090/63767088-9861a180-c907-11e9-84a2-ff25db136abd.png" style="width: 60%" /></p>

<p>위의 공격은 모든 클레스에 대해서 효과적인것은 아니었으나, 다섯개의 이미지에서 2개 정도로 target image로 인식하게 만들 수 있었다.</p>

<h2 id="conclusion">Conclusion</h2>

<ul>
  <li>
    <h3 id="surfacing-inner-properties-of-models">Surfacing Inner Properties of Models</h3>
  </li>
  <li>
    <h3 id="new-interfaces">New interfaces</h3>

    <ul>
      <li>using AI to augment Human intelligence: 인간지능 보조</li>
      <li>이미지의 알파벳처럼 activation을 조합할 수 있다.</li>
      <li>classification 모델을 generative model처럼 …</li>
      <li>Style transfer</li>
      <li>Query large image datasets</li>
      <li>Histogram</li>
      <li>새로운 데이터 셋 탐색</li>
    </ul>
  </li>
</ul>

<h4 id="reference">reference</h4>

<ul>
  <li>https://distill.pub/2019/activation-atlas/</li>
</ul>

  </div><a class="u-url" href="/FastPages/2019/08/26/activation-atlas.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FastPages/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">RoundTable</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">RoundTable</li><li><a class="u-email" href="mailto:ryu071511@gmail.com">ryu071511@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/RRoundTable"><svg class="social svg-icon"><use xlink:href="/FastPages/assets/minima-social-icons.svg#github"></use></svg> <span class="username">RRoundTable</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Tech Blog for RoundTable</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
